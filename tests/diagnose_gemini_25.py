#!/usr/bin/env python3
"""
è¯Šæ–­Gemini 2.5æ¨¡å‹é—®é¢˜
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / ".env", override=True)

def test_gemini_models():
    """æµ‹è¯•ä¸åŒçš„Geminiæ¨¡å‹"""
    print("ğŸ§ª è¯Šæ–­Geminiæ¨¡å‹é—®é¢˜")
    print("=" * 60)
    
    models_to_test = [
        "gemini-2.5-pro",
        "gemini-2.5-flash", 
        "gemini-2.0-flash",
        "gemini-1.5-pro",
        "gemini-1.5-flash"
    ]
    
    google_api_key = os.getenv('GOOGLE_API_KEY')
    if not google_api_key:
        print("âŒ Google APIå¯†é’¥æœªé…ç½®")
        return
    
    print(f"âœ… Google APIå¯†é’¥å·²é…ç½®: {google_api_key[:20]}...")
    
    working_models = []
    
    for model_name in models_to_test:
        print(f"\nğŸ” æµ‹è¯•æ¨¡å‹: {model_name}")
        print("-" * 40)
        
        try:
            # æµ‹è¯•ç›´æ¥API
            print("ğŸ“ æµ‹è¯•ç›´æ¥Google API...")
            import google.generativeai as genai
            genai.configure(api_key=google_api_key)
            
            model = genai.GenerativeModel(model_name)
            response = model.generate_content("è¯·ç”¨ä¸­æ–‡è¯´ï¼šä½ å¥½ï¼Œæˆ‘æ˜¯Geminiæ¨¡å‹")
            
            if response and response.text:
                print(f"âœ… ç›´æ¥APIæˆåŠŸ: {response.text[:100]}...")
                direct_success = True
            else:
                print("âŒ ç›´æ¥APIå¤±è´¥ï¼šæ— å“åº”")
                direct_success = False
                
        except Exception as e:
            print(f"âŒ ç›´æ¥APIå¤±è´¥: {e}")
            direct_success = False
        
        try:
            # æµ‹è¯•LangChain
            print("ğŸ“ æµ‹è¯•LangChainé›†æˆ...")
            from langchain_google_genai import ChatGoogleGenerativeAI
            
            llm = ChatGoogleGenerativeAI(
                model=model_name,
                temperature=0.1,
                max_tokens=200,
                google_api_key=google_api_key
            )
            
            response = llm.invoke("è¯·ç”¨ä¸­æ–‡ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
            
            if response and response.content:
                print(f"âœ… LangChainæˆåŠŸ: {response.content[:100]}...")
                langchain_success = True
            else:
                print("âŒ LangChainå¤±è´¥ï¼šæ— å“åº”")
                langchain_success = False
                
        except Exception as e:
            print(f"âŒ LangChainå¤±è´¥: {e}")
            langchain_success = False
        
        # è®°å½•ç»“æœ
        if direct_success or langchain_success:
            working_models.append({
                'name': model_name,
                'direct': direct_success,
                'langchain': langchain_success
            })
            print(f"âœ… {model_name} éƒ¨åˆ†æˆ–å®Œå…¨å¯ç”¨")
        else:
            print(f"âŒ {model_name} å®Œå…¨ä¸å¯ç”¨")
    
    return working_models

def test_best_working_model(working_models):
    """æµ‹è¯•æœ€ä½³å¯ç”¨æ¨¡å‹"""
    if not working_models:
        print("\nâŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹")
        return None
    
    # é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼ˆä¼˜å…ˆé€‰æ‹©2.5ç‰ˆæœ¬ï¼Œç„¶åæ˜¯LangChainå¯ç”¨çš„ï¼‰
    best_model = None
    for model in working_models:
        if model['langchain']:  # LangChainå¯ç”¨
            if '2.5' in model['name']:  # ä¼˜å…ˆ2.5ç‰ˆæœ¬
                best_model = model['name']
                break
            elif best_model is None:  # å¦‚æœè¿˜æ²¡æœ‰é€‰æ‹©ï¼Œå°±é€‰è¿™ä¸ª
                best_model = model['name']
    
    if best_model is None:
        # å¦‚æœæ²¡æœ‰LangChainå¯ç”¨çš„ï¼Œé€‰æ‹©ç›´æ¥APIå¯ç”¨çš„
        for model in working_models:
            if model['direct']:
                best_model = model['name']
                break
    
    if best_model:
        print(f"\nğŸ¯ é€‰æ‹©æœ€ä½³æ¨¡å‹è¿›è¡Œè¯¦ç»†æµ‹è¯•: {best_model}")
        print("=" * 60)
        
        try:
            from langchain_google_genai import ChatGoogleGenerativeAI
            
            llm = ChatGoogleGenerativeAI(
                model=best_model,
                temperature=0.1,
                max_tokens=800,
                google_api_key=os.getenv('GOOGLE_API_KEY')
            )
            
            # æµ‹è¯•è‚¡ç¥¨åˆ†æ
            print("ğŸ“Š æµ‹è¯•è‚¡ç¥¨åˆ†æèƒ½åŠ›...")
            response = llm.invoke("""
            è¯·ç”¨ä¸­æ–‡åˆ†æè‹¹æœå…¬å¸(AAPL)çš„æŠ•èµ„ä»·å€¼ã€‚
            è¯·ç®€è¦åˆ†æï¼š
            1. å…¬å¸ä¼˜åŠ¿
            2. ä¸»è¦é£é™©
            3. æŠ•èµ„å»ºè®®
            """)
            
            if response and response.content and len(response.content) > 100:
                print("âœ… è‚¡ç¥¨åˆ†ææµ‹è¯•æˆåŠŸ")
                print(f"   å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")
                print(f"   å“åº”é¢„è§ˆ: {response.content[:200]}...")
                return best_model
            else:
                print("âŒ è‚¡ç¥¨åˆ†ææµ‹è¯•å¤±è´¥")
                return None
                
        except Exception as e:
            print(f"âŒ è¯¦ç»†æµ‹è¯•å¤±è´¥: {e}")
            return None
    
    return None

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª Geminiæ¨¡å‹è¯Šæ–­")
    print("=" * 70)
    
    # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
    working_models = test_gemini_models()
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    print("=" * 50)
    
    if working_models:
        print(f"âœ… æ‰¾åˆ° {len(working_models)} ä¸ªå¯ç”¨æ¨¡å‹:")
        for model in working_models:
            direct_status = "âœ…" if model['direct'] else "âŒ"
            langchain_status = "âœ…" if model['langchain'] else "âŒ"
            print(f"   {model['name']}: ç›´æ¥API {direct_status} | LangChain {langchain_status}")
        
        # æµ‹è¯•æœ€ä½³æ¨¡å‹
        best_model = test_best_working_model(working_models)
        
        if best_model:
            print(f"\nğŸ‰ æ¨èä½¿ç”¨æ¨¡å‹: {best_model}")
            print(f"\nğŸ’¡ é…ç½®å»ºè®®:")
            print(f"   1. åœ¨Webç•Œé¢ä¸­é€‰æ‹©'Google'ä½œä¸ºLLMæä¾›å•†")
            print(f"   2. ä½¿ç”¨æ¨¡å‹åç§°: {best_model}")
            print(f"   3. è¯¥æ¨¡å‹å·²é€šè¿‡è‚¡ç¥¨åˆ†ææµ‹è¯•")
        else:
            print(f"\nâš ï¸ è™½ç„¶æ‰¾åˆ°å¯ç”¨æ¨¡å‹ï¼Œä½†è¯¦ç»†æµ‹è¯•å¤±è´¥")
            print(f"   å»ºè®®ä½¿ç”¨: {working_models[0]['name']}")
    else:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯ç”¨çš„Geminiæ¨¡å‹")
        print("ğŸ’¡ å¯èƒ½çš„åŸå› :")
        print("   1. APIå¯†é’¥æƒé™ä¸è¶³")
        print("   2. ç½‘ç»œè¿æ¥é—®é¢˜")
        print("   3. æ¨¡å‹åç§°å·²æ›´æ–°")
        print("   4. APIé…é¢é™åˆ¶")

if __name__ == "__main__":
    main()
