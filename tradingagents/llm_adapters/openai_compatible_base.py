"""
OpenAIå…¼å®¹é€‚é…å™¨åŸºç±»
ä¸ºæ‰€æœ‰æ”¯æŒOpenAIæ¥å£çš„LLMæä¾›å•†æä¾›ç»Ÿä¸€çš„åŸºç¡€å®ç°
"""

import os
import time
from typing import Any, Dict, List, Optional, Union
from langchain_core.messages import BaseMessage
from langchain_core.outputs import ChatResult
from langchain_openai import ChatOpenAI
from langchain_core.callbacks import CallbackManagerForLLMRun

# å¯¼å…¥tokenè·Ÿè¸ªå™¨
try:
    from tradingagents.config.config_manager import token_tracker
    TOKEN_TRACKING_ENABLED = True
except ImportError:
    TOKEN_TRACKING_ENABLED = False


class OpenAICompatibleBase(ChatOpenAI):
    """
    OpenAIå…¼å®¹é€‚é…å™¨åŸºç±»
    ä¸ºæ‰€æœ‰æ”¯æŒOpenAIæ¥å£çš„LLMæä¾›å•†æä¾›ç»Ÿä¸€å®ç°
    """
    
    def __init__(
        self,
        provider_name: str,
        model: str,
        api_key_env_var: str,
        base_url: str,
        api_key: Optional[str] = None,
        temperature: float = 0.1,
        max_tokens: Optional[int] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–OpenAIå…¼å®¹é€‚é…å™¨
        
        Args:
            provider_name: æä¾›å•†åç§° (å¦‚: "deepseek", "dashscope")
            model: æ¨¡å‹åç§°
            api_key_env_var: APIå¯†é’¥ç¯å¢ƒå˜é‡å
            base_url: APIåŸºç¡€URL
            api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è·å–
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
        """
        
        self.provider_name = provider_name
        self.model_name = model
        
        # è·å–APIå¯†é’¥
        if api_key is None:
            api_key = os.getenv(api_key_env_var)
            if not api_key:
                raise ValueError(
                    f"{provider_name} APIå¯†é’¥æœªæ‰¾åˆ°ã€‚"
                    f"è¯·è®¾ç½®{api_key_env_var}ç¯å¢ƒå˜é‡æˆ–ä¼ å…¥api_keyå‚æ•°ã€‚"
                )
        
        # è®¾ç½®OpenAIå…¼å®¹å‚æ•°
        openai_kwargs = {
            "model": model,
            "temperature": temperature,
            "max_tokens": max_tokens,
            **kwargs
        }
        
        # æ ¹æ®LangChainç‰ˆæœ¬ä½¿ç”¨ä¸åŒçš„å‚æ•°å
        try:
            # æ–°ç‰ˆæœ¬LangChain
            openai_kwargs.update({
                "api_key": api_key,
                "base_url": base_url
            })
        except:
            # æ—§ç‰ˆæœ¬LangChain
            openai_kwargs.update({
                "openai_api_key": api_key,
                "openai_api_base": base_url
            })
        
        # åˆå§‹åŒ–çˆ¶ç±»
        super().__init__(**openai_kwargs)
        
        print(f"âœ… {provider_name} OpenAIå…¼å®¹é€‚é…å™¨åˆå§‹åŒ–æˆåŠŸ")
        print(f"   æ¨¡å‹: {model}")
        print(f"   API Base: {base_url}")
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """
        ç”ŸæˆèŠå¤©å“åº”ï¼Œå¹¶è®°å½•tokenä½¿ç”¨é‡
        """
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # è°ƒç”¨çˆ¶ç±»ç”Ÿæˆæ–¹æ³•
        result = super()._generate(messages, stop, run_manager, **kwargs)
        
        # è®°å½•tokenä½¿ç”¨é‡
        if TOKEN_TRACKING_ENABLED:
            try:
                self._track_token_usage(result, kwargs, start_time)
            except Exception as e:
                print(f"âš ï¸ {self.provider_name} Tokenè¿½è¸ªå¤±è´¥: {e}")
        
        return result
    
    def _track_token_usage(self, result: ChatResult, kwargs: Dict, start_time: float):
        """è¿½è¸ªtokenä½¿ç”¨é‡"""
        
        # æå–tokenä½¿ç”¨ä¿¡æ¯
        if hasattr(result, 'llm_output') and result.llm_output:
            token_usage = result.llm_output.get('token_usage', {})
            
            input_tokens = token_usage.get('prompt_tokens', 0)
            output_tokens = token_usage.get('completion_tokens', 0)
            
            if input_tokens > 0 or output_tokens > 0:
                # ç”Ÿæˆä¼šè¯ID
                session_id = kwargs.get('session_id', f"{self.provider_name}_{hash(str(kwargs))%10000}")
                analysis_type = kwargs.get('analysis_type', 'stock_analysis')
                
                # è®°å½•ä½¿ç”¨é‡
                token_tracker.track_usage(
                    provider=self.provider_name,
                    model_name=self.model_name,
                    input_tokens=input_tokens,
                    output_tokens=output_tokens,
                    session_id=session_id,
                    analysis_type=analysis_type
                )
                
                # è®¡ç®—æˆæœ¬
                cost = token_tracker.calculate_cost(
                    provider=self.provider_name,
                    model_name=self.model_name,
                    input_tokens=input_tokens,
                    output_tokens=output_tokens
                )
                
                # è¾“å‡ºä½¿ç”¨ç»Ÿè®¡
                print(f"ğŸ“Š [{self.provider_name.title()}] å®é™…tokenä½¿ç”¨: è¾“å…¥={input_tokens}, è¾“å‡º={output_tokens}")
                if cost > 0:
                    print(f"ğŸ’° [{self.provider_name.title()}] æœ¬æ¬¡è°ƒç”¨æˆæœ¬: Â¥{cost:.6f}")


class ChatDeepSeekOpenAI(OpenAICompatibleBase):
    """DeepSeek OpenAIå…¼å®¹é€‚é…å™¨"""
    
    def __init__(
        self,
        model: str = "deepseek-chat",
        api_key: Optional[str] = None,
        temperature: float = 0.1,
        max_tokens: Optional[int] = None,
        **kwargs
    ):
        super().__init__(
            provider_name="deepseek",
            model=model,
            api_key_env_var="DEEPSEEK_API_KEY",
            base_url="https://api.deepseek.com",
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )


class ChatDashScopeOpenAIUnified(OpenAICompatibleBase):
    """é˜¿é‡Œç™¾ç‚¼ OpenAIå…¼å®¹é€‚é…å™¨ï¼ˆç»Ÿä¸€ç‰ˆæœ¬ï¼‰"""
    
    def __init__(
        self,
        model: str = "qwen-turbo",
        api_key: Optional[str] = None,
        temperature: float = 0.1,
        max_tokens: Optional[int] = None,
        **kwargs
    ):
        super().__init__(
            provider_name="dashscope",
            model=model,
            api_key_env_var="DASHSCOPE_API_KEY",
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )


# æ”¯æŒçš„OpenAIå…¼å®¹æ¨¡å‹é…ç½®
OPENAI_COMPATIBLE_PROVIDERS = {
    "deepseek": {
        "adapter_class": ChatDeepSeekOpenAI,
        "base_url": "https://api.deepseek.com",
        "api_key_env": "DEEPSEEK_API_KEY",
        "models": {
            "deepseek-chat": {"context_length": 32768, "supports_function_calling": True},
            "deepseek-coder": {"context_length": 16384, "supports_function_calling": True}
        }
    },
    "dashscope": {
        "adapter_class": ChatDashScopeOpenAIUnified,
        "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "api_key_env": "DASHSCOPE_API_KEY",
        "models": {
            "qwen-turbo": {"context_length": 8192, "supports_function_calling": True},
            "qwen-plus": {"context_length": 32768, "supports_function_calling": True},
            "qwen-plus-latest": {"context_length": 32768, "supports_function_calling": True},
            "qwen-max": {"context_length": 32768, "supports_function_calling": True},
            "qwen-max-latest": {"context_length": 32768, "supports_function_calling": True}
        }
    }
}


def create_openai_compatible_llm(
    provider: str,
    model: str,
    api_key: Optional[str] = None,
    temperature: float = 0.1,
    max_tokens: Optional[int] = None,
    **kwargs
) -> OpenAICompatibleBase:
    """
    åˆ›å»ºOpenAIå…¼å®¹LLMå®ä¾‹çš„ç»Ÿä¸€å·¥å‚å‡½æ•°
    
    Args:
        provider: æä¾›å•†åç§° ("deepseek", "dashscope")
        model: æ¨¡å‹åç§°
        api_key: APIå¯†é’¥
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§tokenæ•°
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        OpenAIå…¼å®¹çš„LLMå®ä¾‹
    """
    
    if provider not in OPENAI_COMPATIBLE_PROVIDERS:
        raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}ã€‚æ”¯æŒçš„æä¾›å•†: {list(OPENAI_COMPATIBLE_PROVIDERS.keys())}")
    
    provider_config = OPENAI_COMPATIBLE_PROVIDERS[provider]
    adapter_class = provider_config["adapter_class"]
    
    return adapter_class(
        model=model,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        **kwargs
    )


def test_openai_compatible_adapters():
    """æµ‹è¯•æ‰€æœ‰OpenAIå…¼å®¹é€‚é…å™¨"""
    
    print("ğŸ§ª æµ‹è¯•OpenAIå…¼å®¹é€‚é…å™¨")
    print("=" * 50)
    
    for provider_name, config in OPENAI_COMPATIBLE_PROVIDERS.items():
        print(f"\nğŸ”§ æµ‹è¯• {provider_name}...")
        
        try:
            # è·å–ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
            first_model = list(config["models"].keys())[0]
            
            # åˆ›å»ºé€‚é…å™¨
            llm = create_openai_compatible_llm(
                provider=provider_name,
                model=first_model,
                max_tokens=100
            )
            
            print(f"âœ… {provider_name} é€‚é…å™¨åˆ›å»ºæˆåŠŸ")
            
            # æµ‹è¯•å·¥å…·ç»‘å®š
            from langchain_core.tools import tool
            
            @tool
            def test_tool(text: str) -> str:
                """æµ‹è¯•å·¥å…·"""
                return f"å·¥å…·è¿”å›: {text}"
            
            llm_with_tools = llm.bind_tools([test_tool])
            print(f"âœ… {provider_name} å·¥å…·ç»‘å®šæˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ {provider_name} æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    test_openai_compatible_adapters()
