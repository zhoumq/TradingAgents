#!/usr/bin/env python3
"""
è‚¡ç¥¨æ•°æ®ç¼“å­˜ç®¡ç†å™¨
æ”¯æŒæœ¬åœ°ç¼“å­˜è‚¡ç¥¨æ•°æ®ï¼Œå‡å°‘APIè°ƒç”¨ï¼Œæé«˜å“åº”é€Ÿåº¦
"""

import os
import json
import pickle
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, Any, Union
import hashlib


class StockDataCache:
    """è‚¡ç¥¨æ•°æ®ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = None):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸º tradingagents/dataflows/data_cache
        """
        if cache_dir is None:
            # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
            current_dir = Path(__file__).parent
            cache_dir = current_dir / "data_cache"
        
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.stock_data_dir = self.cache_dir / "stock_data"
        self.news_data_dir = self.cache_dir / "news_data"
        self.fundamentals_dir = self.cache_dir / "fundamentals"
        self.metadata_dir = self.cache_dir / "metadata"
        
        for dir_path in [self.stock_data_dir, self.news_data_dir, self.fundamentals_dir, self.metadata_dir]:
            dir_path.mkdir(exist_ok=True)
        
        print(f"ğŸ“ ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜ç›®å½•: {self.cache_dir}")
    
    def _generate_cache_key(self, data_type: str, symbol: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å‚æ•°çš„å­—ç¬¦ä¸²
        params_str = f"{data_type}_{symbol}"
        for key, value in sorted(kwargs.items()):
            params_str += f"_{key}_{value}"
        
        # ä½¿ç”¨MD5ç”ŸæˆçŸ­çš„å”¯ä¸€æ ‡è¯†
        cache_key = hashlib.md5(params_str.encode()).hexdigest()[:12]
        return f"{symbol}_{data_type}_{cache_key}"
    
    def _get_cache_path(self, data_type: str, cache_key: str, file_format: str = "json") -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        if data_type == "stock_data":
            base_dir = self.stock_data_dir
        elif data_type == "news":
            base_dir = self.news_data_dir
        elif data_type == "fundamentals":
            base_dir = self.fundamentals_dir
        else:
            base_dir = self.cache_dir
        
        return base_dir / f"{cache_key}.{file_format}"
    
    def _get_metadata_path(self, cache_key: str) -> Path:
        """è·å–å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„"""
        return self.metadata_dir / f"{cache_key}_meta.json"
    
    def _save_metadata(self, cache_key: str, metadata: Dict[str, Any]):
        """ä¿å­˜å…ƒæ•°æ®"""
        metadata_path = self._get_metadata_path(cache_key)
        metadata['cached_at'] = datetime.now().isoformat()
        
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    def _load_metadata(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """åŠ è½½å…ƒæ•°æ®"""
        metadata_path = self._get_metadata_path(cache_key)
        if not metadata_path.exists():
            return None
        
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}")
            return None
    
    def is_cache_valid(self, cache_key: str, max_age_hours: int = 24) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        metadata = self._load_metadata(cache_key)
        if not metadata:
            return False
        
        cached_at = datetime.fromisoformat(metadata['cached_at'])
        age = datetime.now() - cached_at
        
        return age.total_seconds() < max_age_hours * 3600
    
    def save_stock_data(self, symbol: str, data: Union[pd.DataFrame, str], 
                       start_date: str = None, end_date: str = None, 
                       data_source: str = "unknown") -> str:
        """
        ä¿å­˜è‚¡ç¥¨æ•°æ®åˆ°ç¼“å­˜
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data: è‚¡ç¥¨æ•°æ®ï¼ˆDataFrameæˆ–å­—ç¬¦ä¸²ï¼‰
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            data_source: æ•°æ®æºï¼ˆå¦‚ "tdx", "yfinance", "finnhub"ï¼‰
        
        Returns:
            cache_key: ç¼“å­˜é”®
        """
        cache_key = self._generate_cache_key("stock_data", symbol, 
                                           start_date=start_date, 
                                           end_date=end_date,
                                           source=data_source)
        
        # ä¿å­˜æ•°æ®
        if isinstance(data, pd.DataFrame):
            cache_path = self._get_cache_path("stock_data", cache_key, "csv")
            data.to_csv(cache_path, index=True)
        else:
            cache_path = self._get_cache_path("stock_data", cache_key, "txt")
            with open(cache_path, 'w', encoding='utf-8') as f:
                f.write(str(data))
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'symbol': symbol,
            'data_type': 'stock_data',
            'start_date': start_date,
            'end_date': end_date,
            'data_source': data_source,
            'file_path': str(cache_path),
            'file_format': 'csv' if isinstance(data, pd.DataFrame) else 'txt'
        }
        self._save_metadata(cache_key, metadata)
        
        print(f"ğŸ’¾ è‚¡ç¥¨æ•°æ®å·²ç¼“å­˜: {symbol} ({data_source}) -> {cache_key}")
        return cache_key
    
    def load_stock_data(self, cache_key: str) -> Optional[Union[pd.DataFrame, str]]:
        """ä»ç¼“å­˜åŠ è½½è‚¡ç¥¨æ•°æ®"""
        metadata = self._load_metadata(cache_key)
        if not metadata:
            return None
        
        cache_path = Path(metadata['file_path'])
        if not cache_path.exists():
            return None
        
        try:
            if metadata['file_format'] == 'csv':
                return pd.read_csv(cache_path, index_col=0)
            else:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    return f.read()
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
            return None
    
    def find_cached_stock_data(self, symbol: str, start_date: str = None, 
                              end_date: str = None, data_source: str = None,
                              max_age_hours: int = 24) -> Optional[str]:
        """
        æŸ¥æ‰¾åŒ¹é…çš„ç¼“å­˜æ•°æ®
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            data_source: æ•°æ®æº
            max_age_hours: æœ€å¤§ç¼“å­˜æ—¶é—´ï¼ˆå°æ—¶ï¼‰
        
        Returns:
            cache_key: å¦‚æœæ‰¾åˆ°æœ‰æ•ˆç¼“å­˜åˆ™è¿”å›ç¼“å­˜é”®ï¼Œå¦åˆ™è¿”å›None
        """
        # ç”ŸæˆæŸ¥æ‰¾é”®
        search_key = self._generate_cache_key("stock_data", symbol,
                                            start_date=start_date,
                                            end_date=end_date,
                                            source=data_source)
        
        # æ£€æŸ¥ç²¾ç¡®åŒ¹é…
        if self.is_cache_valid(search_key, max_age_hours):
            print(f"ğŸ¯ æ‰¾åˆ°ç²¾ç¡®åŒ¹é…çš„ç¼“å­˜: {symbol} -> {search_key}")
            return search_key
        
        # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼ŒæŸ¥æ‰¾éƒ¨åˆ†åŒ¹é…ï¼ˆç›¸åŒè‚¡ç¥¨ä»£ç çš„å…¶ä»–ç¼“å­˜ï¼‰
        for metadata_file in self.metadata_dir.glob(f"*_meta.json"):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                if (metadata.get('symbol') == symbol and 
                    metadata.get('data_type') == 'stock_data' and
                    (data_source is None or metadata.get('data_source') == data_source)):
                    
                    cache_key = metadata_file.stem.replace('_meta', '')
                    if self.is_cache_valid(cache_key, max_age_hours):
                        print(f"ğŸ“‹ æ‰¾åˆ°éƒ¨åˆ†åŒ¹é…çš„ç¼“å­˜: {symbol} -> {cache_key}")
                        return cache_key
            except Exception:
                continue
        
        print(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆç¼“å­˜: {symbol}")
        return None
    
    def save_news_data(self, symbol: str, news_data: str, 
                      start_date: str = None, end_date: str = None,
                      data_source: str = "unknown") -> str:
        """ä¿å­˜æ–°é—»æ•°æ®åˆ°ç¼“å­˜"""
        cache_key = self._generate_cache_key("news", symbol,
                                           start_date=start_date,
                                           end_date=end_date,
                                           source=data_source)
        
        cache_path = self._get_cache_path("news", cache_key, "txt")
        with open(cache_path, 'w', encoding='utf-8') as f:
            f.write(news_data)
        
        metadata = {
            'symbol': symbol,
            'data_type': 'news',
            'start_date': start_date,
            'end_date': end_date,
            'data_source': data_source,
            'file_path': str(cache_path),
            'file_format': 'txt'
        }
        self._save_metadata(cache_key, metadata)
        
        print(f"ğŸ“° æ–°é—»æ•°æ®å·²ç¼“å­˜: {symbol} ({data_source}) -> {cache_key}")
        return cache_key
    
    def save_fundamentals_data(self, symbol: str, fundamentals_data: str,
                              data_source: str = "unknown") -> str:
        """ä¿å­˜åŸºæœ¬é¢æ•°æ®åˆ°ç¼“å­˜"""
        cache_key = self._generate_cache_key("fundamentals", symbol,
                                           source=data_source,
                                           date=datetime.now().strftime("%Y-%m-%d"))
        
        cache_path = self._get_cache_path("fundamentals", cache_key, "txt")
        with open(cache_path, 'w', encoding='utf-8') as f:
            f.write(fundamentals_data)
        
        metadata = {
            'symbol': symbol,
            'data_type': 'fundamentals',
            'data_source': data_source,
            'file_path': str(cache_path),
            'file_format': 'txt'
        }
        self._save_metadata(cache_key, metadata)
        
        print(f"ğŸ’¼ åŸºæœ¬é¢æ•°æ®å·²ç¼“å­˜: {symbol} ({data_source}) -> {cache_key}")
        return cache_key
    
    def clear_old_cache(self, max_age_days: int = 7):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        cutoff_time = datetime.now() - timedelta(days=max_age_days)
        cleared_count = 0
        
        for metadata_file in self.metadata_dir.glob("*_meta.json"):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                cached_at = datetime.fromisoformat(metadata['cached_at'])
                if cached_at < cutoff_time:
                    # åˆ é™¤æ•°æ®æ–‡ä»¶
                    data_file = Path(metadata['file_path'])
                    if data_file.exists():
                        data_file.unlink()
                    
                    # åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶
                    metadata_file.unlink()
                    cleared_count += 1
                    
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†ç¼“å­˜æ—¶å‡ºé”™: {e}")
        
        print(f"ğŸ§¹ å·²æ¸…ç† {cleared_count} ä¸ªè¿‡æœŸç¼“å­˜æ–‡ä»¶")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_files': 0,
            'stock_data_count': 0,
            'news_count': 0,
            'fundamentals_count': 0,
            'total_size_mb': 0
        }
        
        for metadata_file in self.metadata_dir.glob("*_meta.json"):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                data_type = metadata.get('data_type', 'unknown')
                if data_type == 'stock_data':
                    stats['stock_data_count'] += 1
                elif data_type == 'news':
                    stats['news_count'] += 1
                elif data_type == 'fundamentals':
                    stats['fundamentals_count'] += 1
                
                # è®¡ç®—æ–‡ä»¶å¤§å°
                data_file = Path(metadata['file_path'])
                if data_file.exists():
                    stats['total_size_mb'] += data_file.stat().st_size / (1024 * 1024)
                
                stats['total_files'] += 1
                
            except Exception:
                continue
        
        stats['total_size_mb'] = round(stats['total_size_mb'], 2)
        return stats


# å…¨å±€ç¼“å­˜å®ä¾‹
_cache_instance = None

def get_cache() -> StockDataCache:
    """è·å–å…¨å±€ç¼“å­˜å®ä¾‹"""
    global _cache_instance
    if _cache_instance is None:
        _cache_instance = StockDataCache()
    return _cache_instance
