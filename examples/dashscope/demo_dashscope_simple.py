#!/usr/bin/env python3
"""
TradingAgents ç®€åŒ–æ¼”ç¤ºè„šæœ¬ - ä½¿ç”¨é˜¿é‡Œç™¾ç‚¼å¤§æ¨¡å‹
è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨é˜¿é‡Œç™¾ç‚¼å¤§æ¨¡å‹è¿›è¡Œç®€å•çš„LLMæµ‹è¯•
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

def test_simple_llm():
    """æµ‹è¯•ç®€å•çš„LLMè°ƒç”¨"""
    print("ğŸš€ é˜¿é‡Œç™¾ç‚¼å¤§æ¨¡å‹ç®€å•æµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥APIå¯†é’¥
    dashscope_key = os.getenv('DASHSCOPE_API_KEY')
    
    if not dashscope_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    print(f"âœ… é˜¿é‡Œç™¾ç‚¼ API å¯†é’¥: {dashscope_key[:10]}...")
    print()
    
    try:
        from tradingagents.llm_adapters import ChatDashScope
        from langchain_core.messages import HumanMessage
        
        print("ğŸ¤– æ­£åœ¨åˆå§‹åŒ–é˜¿é‡Œç™¾ç‚¼æ¨¡å‹...")
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        llm = ChatDashScope(
            model="qwen-plus",
            temperature=0.1,
            max_tokens=1000
        )
        
        print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ!")
        print()
        
        # æµ‹è¯•é‡‘èåˆ†æèƒ½åŠ›
        print("ğŸ“ˆ æµ‹è¯•é‡‘èåˆ†æèƒ½åŠ›...")
        
        messages = [HumanMessage(content="""
è¯·åˆ†æè‹¹æœå…¬å¸(AAPL)çš„æŠ•èµ„ä»·å€¼ï¼Œä»ä»¥ä¸‹å‡ ä¸ªè§’åº¦ï¼š
1. å…¬å¸åŸºæœ¬é¢
2. æŠ€æœ¯é¢åˆ†æ
3. å¸‚åœºå‰æ™¯
4. é£é™©å› ç´ 
5. æŠ•èµ„å»ºè®®

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå¹¶ä¿æŒä¸“ä¸šå’Œå®¢è§‚ã€‚
""")]
        
        print("â³ æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        response = llm.invoke(messages)
        
        print("ğŸ¯ åˆ†æç»“æœ:")
        print("=" * 50)
        print(response.content)
        print("=" * 50)
        
        print("âœ… æµ‹è¯•å®Œæˆ!")
        print()
        print("ğŸŒŸ é˜¿é‡Œç™¾ç‚¼å¤§æ¨¡å‹ç‰¹è‰²:")
        print("  - ä¸­æ–‡ç†è§£èƒ½åŠ›å¼º")
        print("  - é‡‘èé¢†åŸŸçŸ¥è¯†ä¸°å¯Œ")
        print("  - æ¨ç†èƒ½åŠ›å‡ºè‰²")
        print("  - å“åº”é€Ÿåº¦å¿«")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        print("ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()

def test_multiple_models():
    """æµ‹è¯•å¤šä¸ªæ¨¡å‹"""
    print("\nğŸ”„ æµ‹è¯•ä¸åŒçš„é€šä¹‰åƒé—®æ¨¡å‹")
    print("=" * 50)
    
    models = [
        ("qwen-turbo", "é€šä¹‰åƒé—® Turbo - å¿«é€Ÿå“åº”"),
        ("qwen-plus", "é€šä¹‰åƒé—® Plus - å¹³è¡¡æ€§èƒ½"),
        ("qwen-max", "é€šä¹‰åƒé—® Max - æœ€å¼ºæ€§èƒ½")
    ]
    
    question = "è¯·ç”¨ä¸€å¥è¯æ€»ç»“è‹¹æœå…¬å¸çš„æ ¸å¿ƒç«äº‰ä¼˜åŠ¿ã€‚"
    
    for model_id, model_name in models:
        try:
            print(f"\nğŸ§  æµ‹è¯• {model_name}...")
            
            from tradingagents.llm_adapters import ChatDashScope
            from langchain_core.messages import HumanMessage
            
            llm = ChatDashScope(model=model_id, temperature=0.1, max_tokens=200)
            response = llm.invoke([HumanMessage(content=question)])
            
            print(f"âœ… {model_name}: {response.content}")
            
        except Exception as e:
            print(f"âŒ {model_name} æµ‹è¯•å¤±è´¥: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    test_simple_llm()
    test_multiple_models()
    
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("  1. å¦‚æœæµ‹è¯•æˆåŠŸï¼Œè¯´æ˜é˜¿é‡Œç™¾ç‚¼é›†æˆæ­£å¸¸")
    print("  2. å®Œæ•´çš„TradingAgentséœ€è¦è§£å†³è®°å¿†ç³»ç»Ÿçš„å…¼å®¹æ€§")
    print("  3. å¯ä»¥è€ƒè™‘ä¸ºé˜¿é‡Œç™¾ç‚¼æ·»åŠ åµŒå…¥æ¨¡å‹æ”¯æŒ")

if __name__ == "__main__":
    main()
